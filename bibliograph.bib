
@comment{----------------Compression-----------------------------}


@article{Elias:1975:codeword, 
	author    = {P. Elias}, 
	journal   = {IEEE Transactions on Information Theory}, 
	title     = {Universal codeword sets and representations of the integers},
	year      = {1975}, 
	volume    = {21}, 
	number    = {2}, 
	pages     = {194-203}, 
	keywords  = {Source coding;Variable-length coding (VLC)}, 
	doi       = {10.1109/TIT.1975.1055349},
	ISSN      = {0018-9448}, 
	month     = {March},
	annote    = {The author provides a couple of encodings for integers, Gamma coding and Delta coding.  These codings require fewer bits with smaller integers.  This in the integer compression used in one of the best compression methods.}
}

@article{Huffman:1952:coding,
	author    = {Huffman, David A.},
	title     = {A Method for the Construction of Minimum-Redundancy Codes},
	journal   = {Proceedings of the Institute of Radio Engineers},
	month     = {September},
	number    = 9,
	pages     = {1098-1101},
	volume    = 40, 
	year      = 1952,
	annote    = {Original description of huffman coding}
}

@misc{Schindler:1998:huffman-decode,
	author    = {Michael Schindler},
	title     = {{Practical Huffman coding}},
	publisher = {Compression Consulting Schindler},
	month     = {Oct},
	year      = {1998},
	url       = {http://www.compressconsult.com/huffman/},
	note      = {Accessed: 2019-02-07},
	annote    = {Describes chained lookup tables for decoding huffman codes}
}




@inproceedings{Lawlor:2013:compression,
	author    = {Lawlor, Orion Sky},
	title     = {In-memory Data Compression for Sparse Matrices},
	booktitle = {Proceedings of the 3rd Workshop on Irregular Applications: Architectures and Algorithms},
	series    = {IA3 '13},
	year      = {2013},
	isbn      = {978-1-4503-2503-5},
	location  = {Denver, Colorado},
	pages     = {6:1--6:6},
	articleno = {6},
	numpages  = {6},
	url       = {http://doi.acm.org/10.1145/2535753.2535758},
	doi       = {10.1145/2535753.2535758},
	acmid     = {2535758},
	publisher = {ACM},
	address   = {New York, NY, USA},
	annote    = {The author presents an integer compression technique to use on the indices of sparse matrices.  The compression consisists of leading "op codes" that indicate how many bits of data in the number, followed by the number.}
}


@inproceedings{Di:2016:SZ, 
	author    = {S. Di and F. Cappello}, 
	title     = {{Fast Error-Bounded Lossy HPC Data Compression with SZ}},
	booktitle = {2016 IEEE International Parallel and Distributed Processing Symposium}, 
	month     = {May}, 
	year      = {2016}, 
	volume    = {}, 
	number    = {}, 
	pages     = {730-739}, 
	keywords  = {curve fitting;data compression;optimisation;parallel processing;high performance computing;HPC data compression optimization;Squeeze;SZ;curve fitting model;error-bounded lossy HPC data compression;multidimensional snapshot data linearization;Data models;Data compression;Arrays;Predictive models;Solid modeling;Atmospheric modeling;Magnetohydrodynamics}, 
	doi       = {10.1109/IPDPS.2016.11}, 
	ISSN      = {1530-2075},
	annote    = {The authors present a compression for floating point numbers.  The compressed values are stored based on preceeding values, or uncompressed if unable to compute the value with in tolerance.  The paper uses Uncompressed, Previous Value, Linear Interpolation and Quadratic Interpolation as the compression modes, but the general design works with other sets of compression modes.  This is the basis for the best performing compression method.}
}


@article{Lindstrom:2014:zfp, 
	author    = {P. Lindstrom}, 
	journal   = {IEEE Transactions on Visualization and Computer Graphics}, 
	title     = {Fixed-Rate Compressed Floating-Point Arrays}, 
	month     = {Dec},
	year      = {2014}, 
	volume    = {20}, 
	number    = {12}, 
	pages     = {2674-2683}, 
	keywords  = {computer graphics;data compression;data visualisation;embedded systems;floating point arithmetic;graphics processing units;random-access storage;storage management;fixed-rate compressed floating-point arrays;floating-point data;fixed-precision values;variable-length bit stream;memory management;near-lossless compression scheme;read and write random access;fixed-rate texture compression methods;graphics hardware;scientific applications;orthogonal block transform;embedded coding;per-block bit stream;bit rate selection;data access;software write-back cache;hardware implementation;fixed-point arithmetic operations;lossy compression;quantitative data analysis;numerical simulation;data visualization;Floating-point arithmetic;Image coding;Encoding;Bandwidth allocation;Data visualization;Computational modeling;Data compression;floating-point arrays;orthogonal block transform;embedded coding}, 
	doi       = {10.1109/TVCG.2014.2346458}, 
	ISSN      = {1077-2626},
	annote    = {ZFP compression compresses floating point or integer data.  ZFP compression works in blocks of values.  Additionally there is discussion on compressing n-dimentional data for best effect with ZFP.} 
}


@comment{----------------Solver BG Info--------------------------}

@book{Saad:2003:IterativeMethods,
	author    = {Saad, Y.},
	title     = {Iterative Methods for Sparse Linear Systems},
	year      = {2003},
	isbn      = {0898715342},
	edition   = {2nd},
	publisher = {Society for Industrial and Applied Mathematics},
	address   = {Philadelphia, PA, USA},
	annote    = {A textbook on iterative methods for sparse linear systems.  In particular, it contains a description of CSR format.}
} 

@book{Kincaid:2009:Numerical,
	title     = {Numerical Analysis: Mathematics of Scientific Computing},
	author    = {Kincaid, David R. and Cheney, E. Ward},
	isbn      = {9780821847886},
	lccn      = {2008047389},
	series    = {Pure and applied undergraduate texts},
	year      = {2002},
	publisher = {American Mathematical Society},
	annote    = {A general numerical analysis textbook, including iterative methods}
}

@Article{Goumas:2009:performanceEval,
	author    = {Goumas, Georgios
	             and Kourtis, Kornilios
	             and Anastopoulos, Nikos
	             and Karakasis, Vasileios
	             and Koziris, Nectarios},
	title     = {Performance evaluation of the sparse matrix-vector multiplication on modern architectures},
	journal   = {The Journal of Supercomputing},
	year      = {2009},
	month     = {Oct},
	day       = {01},
	volume    = {50},
	number    = {1},
	pages     = {36-77},
	issn      = {1573-0484},
	doi       = {10.1007/s11227-008-0251-8},
	url       = {https://doi.org/10.1007/s11227-008-0251-8},
	annote    = {An evaluation of the performance of SpMV on modern hardware.  IE memory bound computation.}
}

@techreport{Shewchuk:1994:IntroToCG,
	author    = {Shewchuk, Jonathan R},
	title     = {An Introduction to the Conjugate Gradient Method Without the Agonizing Pain},
	year      = {1994},
	publisher = {Carnegie Mellon University},
	address   = {Pittsburgh, PA, USA},
	institution={Carnegie Mellon University}
} 


@book{Nearing:2010:toolsForPhysics,
	title     = {Mathematical Tools for Physics},
	author    = {Nearing, J.},
	isbn      = {9780486482125},
	lccn      = {2010010191},
	series    = {Dover books on mathematics},
	url       = {https://books.google.com/books?id=eSXcngEACAAJ},
	year      = {2010},
	publisher = {Dover Publications},
	annote    = {Includes n dimentional second derivative test.}
}



@comment{----------------------Misc------------------------------}

@misc{Lindquist:2018:projectGithub,
	title     = {Code for \input{title.txt}},
	author    = {Neil Lindquist},
	howpublished={\url{https://github.com/Collegeville/-ZFP}},
	annote    = {The Github repository containing many of the implementations used for timings}
}

@misc{7cpu:-:Bradwell,
	key       = {Intel Bradwell},
	title     = {{Intel Bradwell}},
	howpublished={\url{https://www.7-cpu.com/cpu/Broadwell.html}},
	note      = {Accessed: 2018-11-06},
	annote    = {Performance specs of walbert's head}
}

@article{Davis:2011:FloridaMatrixCollection,
	author = {Davis, Timothy A. and Hu, Yifan},
	title = {{The University of Florida Sparse Matrix Collection}},
	journal = {ACM Trans. Math. Softw.},
	issue_date = {November 2011},
	volume = {38},
	number = {1},
	month = dec,
	year = {2011},
	issn = {0098-3500},
	pages = {1:1--1:25},
	articleno = {1},
	numpages = {25},
	url = {http://doi.acm.org/10.1145/2049662.2049663},
	doi = {10.1145/2049662.2049663},
	acmid = {2049663},
	publisher = {ACM},
	address   = {New York, NY, USA},
	keywords  = {Graph drawing, multilevel algorithms, performance evaluation, sparse matrices},
	annote    = {A collection of sparse, linear systems from real world problems.}
}


@techreport{Dongarra:2015:HPCG,
	author    = {Jack Dongarra and Michael Heroux and Piotr Luszczek},
	title     = {{HPCG Benchmark: a New Metric for Ranking High Performance Computing Systems}},
	journal   = {University of Tennessee Computer Science Technical Report },
	number    = {UT-EECS-15-736},
	month     = {November},
	year      = {2015},
	institution={Electrical Engineering and Computer Science Department, Knoxville, Tennessee},
	publisher = {University of Tennessee},
	keywords  = {Additive Schwarz, HPC Benchmarking, Multigrid smoothing, Preconditioned Conjugate Gradient, Validation and Verification},
	url       = {http://www.eecs.utk.edu/resources/library/file/1047/ut-eecs-15-736.pdf},
	annote    = {HPCG is the code base used to test different compression techniques.  This is the paper that describes it as a benchmark.  Nuff said.}
}

@book{Higham:1996:ErrorBounds,
	author    = {Higham, Nicholas J.},
	title     = {Accuracy and Stability of Numerical Algorithms},
	year      = {1996},
	isbn      = {0898713552},
	publisher = {Society for Industrial and Applied Mathematics},
	address   = {Philadelphia, PA, USA},
	annote    = {The author takes a detailed look at calculating error bounds
	             for floating point arithmetic and algorithms.  Additionally,
	             there is some discussion on how to minimize error in said
	             algorithms.},
	alcuinref = {QA297 .H53 1996}
}


@article{Claude-Pierre:2013:improvedErrorBound,
	author    = {Claude-Pierre Jeannerod and
	             Siegfried M. Rump},
	title     = {Improved Error Bounds for Inner Products in Floating-Point Arithmetic},
	journal   = {SIAM Journal on Matrix Analysis and Applications},
	volume    = {34},
	number    = {2},
	pages     = {338-344},
	year      = {2013},
	doi       = {10.1137/120894488},
	URL       = {http://dx.doi.org/10.1137/120894488},
	eprint    = {http://dx.doi.org/10.1137/120894488},
	annote    = {The authors improve on the error bound for inner products from Higham's 
	             \textit{Accuracy and Stability of Numerical Algorithms}.}
}


@book{Hennessy:1990:ComputerArchitecture,
	author    = {Hennessy, John L. and Patterson, David A.},
	title     = {Computer Architecture: A Quantitative Approach},
	edition   = {Sixth},
	year      = {2017},
	isbn      = {978-01-28119-05-1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address   = {San Francisco, CA, USA},
	annote    = {Has information about Computer Architecture, including instruction level parallelism}
} 


@comment{---------------------Mixed Prec Info----------------------------------}

@article{Buttari:2008:mixedPrec,
 author     = {Buttari, Alfredo and
               Dongarra, Jack and
               Kurzak, Jakub and
               Luszczek, Piotr and
               Tomov, Stanimir},
 title      = {Using Mixed Precision for Sparse Matrix Computations
               to Enhance the Performance While Achieving
               64-bit Accuracy},
 journal    = {ACM Trans. Math. Softw.},
 issue_date = {July 2008},
 volume     = {34},
 number     = {4},
 month      = jul,
 year       = {2008},
 issn       = {0098-3500},
 pages      = {17:1--17:22},
 articleno  = {17},
 numpages   = {22},
 url        = {http://doi.acm.org/10.1145/1377596.1377597},
 doi        = {10.1145/1377596.1377597},
 acmid      = {1377597},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 keywords   = {Linear systems, floating point, iterative refinement,
               precision},
 annote     = {The authors tried using both single and double precision
               in an iterative refinement algorithms to try to increase speed while retaining accuracy.  The algorithms were modified to use a single precision preconditioner in a double precision iterative solver.}
} 


@inproceedings{Langou:2006:mixPrec,
 author     = {Buttari, Alfredo and
               Dongarra, Jack and
               Langou, Julie and
               Langou, Julien and
               Luszczek, Piotr and
               Kurzak, Jakub},
 title      = {Exploiting the Performance of
               32 Bit Floating Point Arithmetic in
               Obtaining 64 Bit Accuracy
               (Revisiting Iterative Refinement for Linear Systems)},
 booktitle  = {Proceedings of the 2006 ACM/IEEE Conference
               on Supercomputing},
 series     = {SC '06},
 year       = {2006},
 isbn       = {0-7695-2700-0},
 location   = {Tampa, Florida},
 articleno  = {113},
 url        = {http://doi.acm.org/10.1145/1188455.1188573},
 doi        = {10.1145/1188455.1188573},
 acmid      = {1188573},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 annote     = {Single precision floats are able to be processed faster and
               the communication is much cheaper that working with double
               precision floats.  The authors mixed single and double
               precision in each refinment step.  They found that if
               factorization, forward substitution and backward
               substitution are do in single precision while the residual
               and update to the solution are done in double precision
               then the iterative refinement will produce the same
               accuracy than if double precision was used exclusively
               (as long as the matrix is not too badly conditioned).}
} 

@article{Babolin:2008:coursePass,
 author     = {Marc Baboulin and
               Alfredo Buttari and
               Jack Dongarra and
               Jakub Kurzak and
               Julie Langou and
               Julien Langou and
               Piotr Luszczek and
               Stanimire Tomov},
 title      = {Accelerating Scientific Computations
               with Mixed Precision Algorithms},
 journal    = {CoRR},
 volume     = {abs/0808.2794},
 year       = {2008},
 url        = {http://arxiv.org/abs/0808.2794},
 timestamp  = {Mon, 05 Dec 2011 18:04:36 +0100},
 biburl     = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-0808-2794},
 bibsource  = {dblp computer science bibliography, http://dblp.org},
 annote     = {The authors studied performance of iterative linear algebra
               solvers by doing the first few refinement passes with single
               precision floats, before using double precision to get the
               final answer.  They found that doing the first passes with
               single precision floats was significantly faster than using
               only double precision.}
}

@article{Buttari:2007:coursePass,
 author     = {Buttari, Alfredo and
               Dongarra, Jack and
               Langou, Julie and
               Langou, Julien and
               Luszczek, Piotr and
               Kurzak, Jakub},
 title      = {Mixed Precision Iterative Refinement Techniques
               for the Solution of Dense Linear Systems},
 journal    = {Int. J. High Perform. Comput. Appl.},
 issue_date = {November  2007},
 volume     = {21},
 number     = {4},
 month      = nov,
 year       = {2007},
 issn       = {1094-3420},
 pages      = {457--466},
 numpages   = {10},
 url        = {http://dx.doi.org/10.1177/1094342007084026},
 doi        = {10.1177/1094342007084026},
 acmid      = {1297653},
 publisher  = {Sage Publications, Inc.},
 address    = {Thousand Oaks, CA, USA},
 keywords   = {Cholesky factorization, LU, iterative, mixed-precision, refinement},
 annote     = {The authors compared using strictly double precision to
               using mixed single and double precision in iterative
               linear equation solvers with dense matrices.  Except for
               small problems, the mixed precision solver was able to
               solve faster.  The algorithm in question first solved
               the system in single precision then refined it to double
               precision quality.}
}  


@article{Hogg:2010:multiplePasses,
 author     = {Hogg, J. D. and
               Scott, J. A.},
 title      = {A Fast and Robust Mixed-precision Solver for the
               Solution of Sparse Symmetric Linear Systems},
 journal    = {ACM Trans. Math. Softw.},
 issue_date = {April 2010},
 volume     = {37},
 number     = {2},
 month      = apr,
 year       = {2010},
 issn       = {0098-3500},
 pages      = {17:1--17:24},
 articleno  = {17},
 numpages   = {24},
 url        = {http://doi.acm.org/10.1145/1731022.1731027},
 doi        = {10.1145/1731022.1731027},
 acmid      = {1731027},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 keywords   = {FGMRES, Fortran 95, Gaussian elimination,
               iterative refinement, mixed precision,
               multifrontal method, sparse symmetric linear systems},
 annote     = {The author anaylizes uses a mixed precision strategy
               utilizing fallbacks to more accurate algorithms to reach
               the user's requested precision.  It starts with a single
               precision solve of \(Ax=b\), then applies mixed-precision
               iterative refinement, then falls back to mixed-precision
               FGMRES, if target precision is not yet met then it uses 
               a double precision solver on \(Ax=b\) followed by double
               precision iterative refinement, then uses double-precision
               FGMRES.  If target precision is not met a this point, then
               algorithm results in an error.}
} 



@article{Chiang:2017:generalMixedPrec,
 author     = {Chiang, Wei-Fan and
               Baranowski, Mark and
               Briggs, Ian and
               Solovyev, Alexey and
               Gopalakrishnan, Ganesh and
               Rakamari\'{c}, Zvonimir},
 title      = {Rigorous Floating-point Mixed-precision Tuning},
 journal    = {SIGPLAN Not.},
 issue_date = {January 2017},
 volume     = {52},
 number     = {1},
 month      = jan,
 year       = {2017},
 issn       = {0362-1340},
 pages      = {300--315},
 numpages   = {16},
 url        = {http://doi.acm.org/10.1145/3093333.3009846},
 doi        = {10.1145/3093333.3009846},
 acmid      = {3009846},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 keywords   = {Energy-efficient computing, Floating-point arithmetic,
               Precision allocation, Program optimization,
               Rigorous compilation},
 annote     = {The authors address general improvements for mixing
               precision of floats in algorithms (not just iterative refinement
               solvers).}
} 



@article{Demmel:2006:mixedPrecError,
 author     = {Demmel, James and
               Hida, Yozo and
               Kahan, William and
               Li, Xiaoye S. and
               Mukherjee, Sonil and
               Riedy, E. Jason},
 title      = {Error Bounds from Extra-precise Iterative Refinement},
 journal    = {ACM Trans. Math. Softw.},
 issue_date = {June 2006},
 volume     = {32},
 number     = {2},
 month      = jun,
 year       = {2006},
 issn       = {0098-3500},
 pages      = {325--351},
 numpages   = {27},
 url        = {http://doi.acm.org/10.1145/1141885.1141894},
 doi        = {10.1145/1141885.1141894},
 acmid      = {1141894},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 keywords   = {BLAS, LAPACK, Linear algebra, floating-point arithmetic},
 annote     = {The authors address calculating the error bounds when the
               precision used to calculate the residual is higher than
               the precision used for the rest of the calculations in an
               iterative refinement algorithm.}
} 



@Article{Anzt:2010:mixedPrec,
 author     = {Anzt, Hartwig and
               Rocker, Bj{\"o}rn and
               Heuveline, Vincent},
 title      = {Energy efficiency of mixed precision iterative refinement
               methods using hybrid hardware platforms},
 journal    = {Computer Science - Research and Development},
 year       = {2010},
 volume     = {25},
 number     = {3},
 pages      = {141--148},
 doi        = {10.1007/s00450-010-0124-2},
 url        = {http://dx.doi.org/10.1007/s00450-010-0124-2},
 annote     = {Experimental times for using mixed precision solvers on
               high end computers.  The algorithm used utilized single
               precision to solve \(Ac=r\) where \(c\) is the amount to
               increment \(x\).}
}



@article{Tsuchida:2012:generalMixedPrec,
 author     = {Eiji Tsuchida and
               Yoong-Kee Choe},
 title      = {Iterative diagonalization of symmetric matrices in mixed
               precision and its application to electronic structure
               calculations},
 doi        = {10.1016/j.cpc.2012.01.002},
 url        = {https://doi.org/10.1016/j.cpc.2012.01.002},
 year       = {2012},
 month      = {apr},
 publisher  = {Elsevier {BV}},
 volume     = {183},
 number     = {4},
 pages      = {980--985},
 journal    = {Computer Physics Communications},
 annote     = {The author looks at using mixed precision when iteratively
               diagonalizing a symmetric matrix.}
}



@article{Goddeke:2007:mixedPrec,
 author     = {G\"{o}ddeke, Dominik and
               Strzodka, Robert and
               Turek, Stefan},
 title      = {Performance and Accuracy of Hardware-oriented Native-,
               Emulated-and Mixed-precision Solvers in FEM Simulations},
 journal    = {Int. J. Parallel Emerg. Distrib. Syst.},
 issue_date = {July 2007},
 volume     = {22},
 number     = {4},
 month      = jan,
 year       = {2007},
 issn       = {1744-5760},
 pages      = {221--256},
 numpages   = {36},
 url        = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=874CC4C5324572D34BE99A039AFB8849?doi=10.1.1.214.7563&rep=rep1&type=pdf},
 doi        = {10.1080/17445760601122076},
 acmid      = {1392074},
 publisher  = {Taylor \& Francis, Inc.},
 address    = {Bristol, PA, USA},
 keywords   = {Emulated precision, FEM, Graphics hardware,
               Large sparse linear equation systems,
               Mixed precision iterative refinement, Reconfigurable hardware},
 annote     = {The authors address using mixed precision in iterative
               refinement algorithms, as well as emulating higher precision
               with a pair of single precision floats.  The authors also
               discuss the ability of different precisions of floating point
               operations on various hardware.}
} 






@article{Li:2002:extendedPrec,
 author     = {Li, Xiaoye S. and
               Demmel, James W. and
               Bailey, David H. and
               Henry, Greg and
               Hida, Yozo and
               Iskandar, Jimmy and
               Kahan, William and
               Kang, Suh Y. and
               Kapur, Anil and
               Martin, Michael C. and
               Thompson, Brandon J. and
               Tung, Teresa and
               Yoo, Daniel J.},
 title      = {Design, Implementation and Testing of Extended and Mixed Precision BLAS},
 journal    = {ACM Trans. Math. Softw.},
 issue_date = {June 2002},
 volume     = {28},
 number     = {2},
 month      = jun,
 year       = {2002},
 issn       = {0098-3500},
 pages      = {152--205},
 numpages   = {54},
 url        = {http://doi.acm.org/10.1145/567806.567808},
 doi        = {10.1145/567806.567808},
 acmid      = {567808},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 keywords   = {BLAS, double-double arithmetic, extended and mixed precision},
 annote     = {The authors address the design decisions of using higher precision
               internally in BLAS, as well as some related features.  A quote of note:
               ``...Intel processors or their AMD and Cyrix clones, are designed to
               run fastest performing arithmetic to the full width, 80-bits, of their
               internal registers. These computers confer some benefits of wider
               arithmetic at little or no performancepenalty.''}
} 



@inproceedings{Jenkins:2012:droppedBytes,
 author     = {Jenkins, John and
               Schendel, Eric R. and
               Lakshminarasimhan, Sriram and
               Boyuka,II, David A. and 
               Rogers, Terry and 
               Ethier, Stephane and 
               Ross, Robert and 
               Klasky, Scott and 
               Samatova, Nagiza F.},
 title      = {Byte-precision Level of Detail Processing 
               for Variable Precision Analytics},
 booktitle  = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
 series     = {SC '12},
 year       = {2012},
 isbn       = {978-1-4673-0804-5},
 location   = {Salt Lake City, Utah},
 pages      = {48:1--48:11},
 articleno  = {48},
 numpages   = {11},
 url        = {http://dl.acm.org/citation.cfm?id=2388996.2389062},
 acmid      = {2389062},
 publisher  = {IEEE Computer Society Press},
 address    = {Los Alamitos, CA, USA},
 annote     = {The authors are attempting to reduce IO bottlenecks by not 
               sending all bytes of a double-precision float when doing 
               calculations, resulting in a lower precision value.}
} 

@article{Tran-Thong:1977:extendedPrec,
 author     = {Tran-Thong and Liu, Bede},
 title      = {Floating Point Fast Fourier Transform Computation Using
               Double Precision Floating Point Accumulators},
 journal    = {ACM Trans. Math. Softw.},
 issue_date = {March 1977},
 volume     = {3},
 number     = {1},
 month      = mar,
 year       = {1977},
 issn       = {0098-3500},
 pages      = {54--59},
 numpages   = {6},
 url        = {http://doi.acm.org/10.1145/355719.355723},
 doi        = {10.1145/355719.355723},
 acmid      = {355723},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 annote     = {The author looks at using a double precision accumulator when working with
               single precision data in a Fast Fourier Transformation.}
} 


@article{Figueroa:1995:extendedPrecWorthless,
 author     = {Figueroa, Samuel A.},
 title      = {When is Double Rounding Innocuous?},
 journal    = {SIGNUM Newsl.},
 issue_date = {July 1995},
 volume     = {30},
 number     = {3},
 month      = jul,
 year       = {1995},
 issn       = {0163-5778},
 pages      = {21--26},
 numpages   = {6},
 url        = {http://doi.acm.org/10.1145/221332.221334},
 doi        = {10.1145/221332.221334},
 acmid      = {221334},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 annote     = {The author proves that doing an arithmetic operation on single
               precision floats can be simulated by doing them in double precision
               then converting to single precision (I think?)},
} 


@article{Kaneko:1973:singlePrecAccum,
 author     = {Kaneko, Toyohisa and
               Liu, Bede},
 title      = {On Local Roundoff Errors in Floating-Point Arithmetic},
 journal    = {J. ACM},
 issue_date = {July 1973},
 volume     = {20},
 number     = {3},
 month      = jul,
 year       = {1973},
 issn       = {0004-5411},
 pages      = {391--398},
 numpages   = {8},
 url        = {http://doi.acm.org/10.1145/321765.321771},
 doi        = {10.1145/321765.321771},
 acmid      = {321771},
 publisher  = {ACM},
 address    = {New York, NY, USA},
 annote     = {The authors find the bound on error in float addition with a
               single precision accumulator with guard bits. They found that
               accuracy is almost as high as if a double precision accumulator
               had been used.}
} 
