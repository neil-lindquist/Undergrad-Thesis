\documentclass[12pt,]{article}

\usepackage{amsmath}
%\usepackage[nonewpage]{imakeidx}
%\indexsetup{level=\section}
%\usepackage[colorlinks=true,linkcolor=blue]{hyperref}

\newcommand{\mat}[1]{\mathbf{#1}}

%opening
\title{\input{title.txt} \\ \Large Basics of Linear Algebra}
\author{Neil Lindquist}
\date{}

\makeindex

\begin{document}

\maketitle

This project is based on linear algebra\index{linear algebra}, so an basic understanding is useful.
Let's first consider a simple linear system of equations\index{linear system}:
\begin{equation}
	\label{eq:ex-equations}
	\begin{aligned}
		2x + 3y + 4z &= 4 \\
		4x + 3y + 2z &= 16 \\
		1x + 2y + 1z &= 5 \\
	\end{aligned}
\end{equation}
Since there are 3 variables and 3 equations, there is a unique solution, \({x=3}, {y=2}, {z=-2}\).
Linear algebra generalizes solving these types of systems.
First, we will re-write Equations~\ref{eq:ex-equations} by grouping the different parts of the system.
\begin{equation}
\label{eq:ex-matvec}
	\begin{bmatrix}
		2 & 3 & 4 \\ 4 & 3 & 2 \\ 1 & 2 & 1
	\end{bmatrix}
	\begin{bmatrix} x \\ y \\ z \end{bmatrix}
	=
	\begin{bmatrix} 4 \\ 16 \\ 5 \end{bmatrix}
\end{equation}
As you can see, the coefficients from the left hand side are gathered into a 3x3 block, the variables are gathered into a list and the right hand side is gathered into a list.
The fact that the variable list is rotated from the ordering in the original equations might be unintuitive, but gives more power to advanced linear algebra topics.
The block of coefficients is called a matrix\index{matrix}.
The variable column and the right-hand-side column are called vectors\index{vector}.
These objects extends to linear systems with different numbers of equations and variables.
There is much additionally theory and technique for working with linear algebra; however, its all built on these basic linear algebra structures.



% Trimmed information that I didn't want to throw away outright

\iffalse

Now that we have these matrix and vector objects, we should figure out operations with them.
The simplest operation is transpose\index{transpose}.
Written as a superscript T, transpose basically turns rows into columns and columns into rows.
\begin{align*}
	\mat{A}^T &= 
	\begin{bmatrix}
		2 & 3 & 4 \\ 4 & 3 & 2 \\ 1 & 2 & 1
	\end{bmatrix}^T \\
	&= \begin{bmatrix}
		2 & 4 & 1 \\ 3 & 3 & 2 \\ 4 & 2 & 1
	\end{bmatrix}
\end{align*}
Vectors can similarly be transposed, becoming single row matrices called row vectors\index{vector!row}.
Row vectors are less common then column vectors, but come up occasionally.
\begin{align*}
	\vec{b}^T &= \begin{bmatrix} 4 \\ 16 \\ 5 \end{bmatrix} \\
	&= \begin{bmatrix} 4 & 16 & 5 \end{bmatrix}
\end{align*}

The next operation to look at is matrix-vector multiplication\index{matrix-vector multiplication}.
As the name implies, it takes a matrix and a vector then ``multiplies'' them to get another vector.
Matrix-vector multiplication is basically the opposite of the separation we did.
For example, if we know the values of the variables but didn't know what the right hand sides are, we could multiply \(\mat{A}\) by the vector with are variables.
So,
\begin{equation}
	\begin{bmatrix}
		2 & 3 & 4 \\ 4 & 3 & 2 \\ 1 & 2 & 1
	\end{bmatrix}
	\begin{bmatrix}3 \\ 2 \\ -2\end{bmatrix}
	= 
	\begin{bmatrix}
		2\cdot3 + 3\cdot2 + 4\cdot(-2) \\ 4\cdot3 + 3\cdot2 + 2\cdot(-2) \\ 1\cdot3 + 2\cdot2 + 1\cdot(-2)
	\end{bmatrix}
	=
	\begin{bmatrix}4 \\ 16 \\ 5\end{bmatrix}
\end{equation}
Note how for each row, the element in the \(i\)th column was multiplied by the \(i\)th element in the vector then the products for each row were added together.
Additionally, row vectors can be multiplied by matrices, defined by \(\vec{x}^T\mat{A} = (\mat{A}^T\vec{x})^T\).
So, for each matrix column, the element in the \(j\)th row is multiplied by the \(j\)th element in the vector then the products for each column are added together.

Next, there is a group of operations called vector arithmetic\label{vector arithmetic}.
The first part is scalar multiplication, where each element in the vector is multiplied by a scalar.
The second par is vector addition and subtraction, where each element in one vector is added or subtracted by the respective value in the other vector.
For example,
\begin{equation}
	2
	\begin{bmatrix}2 \\ 3 \\ 1\end{bmatrix}
	+
	\begin{bmatrix}-1 \\ 5 \\ 4\end{bmatrix}
	=
	\begin{bmatrix}4 \\ 6 \\ 2\end{bmatrix}
	+
	\begin{bmatrix}-1 \\ 5 \\ 4\end{bmatrix}
	=
	\begin{bmatrix}4 + (-1) \\ 6 + 5 \\ 2 + 4\end{bmatrix}
	=
	\begin{bmatrix}3 \\ 11 \\ 6\end{bmatrix}
\end{equation}

The next operation of note is the dot product\index{dot product}.
As the name implies, dot products are indicated with a dot (\(\cdot\)).
The dot product takes two vectors, multiplies each element of the first with the respective element of the second then totals the result, giving a scalar value.
For example,
\begin{equation}
	\begin{bmatrix}2 \\ 3 \\ 1\end{bmatrix}
	\cdot
	\begin{bmatrix}-1 \\ 5 \\ 4\end{bmatrix}
	=
	2\cdot(-1) + 3\cdot5 + 1\cdot 4
	= 17
\end{equation}
Note that by treating row vectors as single row matrices, the dot product of \(\vec{u}\) and \(\vec{v}\) can be written \(\vec{u}^T\vec{v}\).

The last operation of note is a norm\index{norm}.
Norms take a single vector and give a scalar value that is, in some sense, the magnitude of the vector.
There are many variants, but the L2 norm is defined as \(\|\vec{x}\|_2 = \sqrt{\vec{x}\cdot\vec{x}}\).
Note that for the 2- and 3- dimensional cases, it is equivalent to the straight line distance from (0,0,0) to \(\vec{x}\):
\begin{equation}
	\begin{aligned}
		\sqrt{\vec{x}\cdot\vec{x}}
		&= \sqrt{x^2+y^2+z^2} \\
		&= \sqrt{(x-0)^2 + (y-0)^2 + (z-0)^2} \\
		&= \text{the distance}
	\end{aligned}
\end{equation}

%%%

In linear systems that arise from certain applications, sometimes there are many 0's in the matrix.
Matrices that have lots of 0's are referred to as sparse\index{matrix!sparse}.
Because of the number of 0's, a few improvements can arise.
First, sparse matrices are often stored as just the nonzero values plus position information.
Second, often sparse linear systems\index{linear system!sparse} are solved with what are called iterative linear solvers\index{iterative linear solvers}.

Iterative linear solvers take an initial guess of a solution, then keep improving that solution until close enough to the actual solution.
Note that we want \(\mat{A}\vec{x} = \vec{b}\), and so by subtracting \(\vec{b}\) from both sides we want \(\mat{A}\vec{x} - \vec{b} = \vec{0}\) where \(\vec{0}\) is a vector of all 0's.
So, we can measure how close we are to that by measuring the magnitude of that difference (sometimes called the residual\index{residual}), \(\|\mat{A}\vec{x} - \vec{b}\|_2\), and calling a solution good enough if that norm is smaller than some preset tolerance.
When the solution is good enough, the solver is said to have converged\index{convergence} to the solution.
Note that because the solver keeps refining the solution, there is some margin for inaccuracies in the computation.
However, if the computation is too inaccurate, the rate at which the solver converges with be significantly slower.


\fi

\end{document}
