Conjugate Gradient is the iterative solver used by HPCG~\cite{Dongarra:2015:HPCG}.
Symmetric, positive definite matrices will guarantee the converge of Conjugate Gradient to the correct solution within \(n\) iterations when using exact algebra~\cite{Saad:2003:IterativeMethods}.
As an iterative method, Conjugate Gradient can provide a solution, \(\vec{x}\), where \(\left\|\mat{A}\vec{x}-\vec{b}\right\|\) is within some tolerance, after significantly fewer than \(n\) iterations, allowing it to find solutions to problems where even \(n\) iterations is infeasible~\cite{Shewchuk:1994:IntroToCG}.
%TODO go into more high level details of CG

%Steepest Decent
To understand the Conjugate Gradient, first consider the quadratic form of \(\mat{A}\vec{x} = \vec{b}\).
The quadratic form is a function \(f:\mathbb{R}^n\to\mathbb{R}\) where
\begin{equation}
\label{eq:quad-form}
	f(\vec{x}) = \frac{1}{2}\vec{x}\cdot\left(\mat{A}\vec{x}\right) - \vec{b}\cdot\vec{x}
\end{equation}
for some \(c\in\mathbb{R}\).
Note that
\begin{align*}
	\nabla f\left(\vec{x}\right)
	&= \nabla \left(\frac{1}{2}\vec{x}\cdot\left(\mat{A}\vec{x}\right) - \vec{b}\cdot\vec{x}\right) \\
	&= \frac{1}{2}\nabla \left(\vec{x}\cdot\left(\mat{A}\vec{x}\right)\right) - \nabla\left(\vec{b}\cdot\vec{x}\right) \\
	&= \frac{1}{2}\left(\mat{A}\vec{x}+\mat{A}^T\vec{x}\right)-\vec{b}
\end{align*}
Then, when \(\mat{A}\) is symmetric,
\begin{equation*}
	\nabla f(\vec{x}) = \mat{A}\vec{x} - \vec{b}
\end{equation*}
So, the solution to \(\mat{A}\vec{x} = \vec{b}\) is the sole critical point of \(f\)~\cite{Nearing:2010:toolsForPhysics}.
Since \(\mat{A}\) is the Hessian matrix of \(f\) at the point, if \(\mat{A}\) is positive definite, then that critical point is a minimum.
Thus, if \(\mat{A}\) is a symmetric, positive definite matrix, then the minimum of \(f\) is the solution to \(\mat{A}\vec{x} = \vec{b}\)~\cite{Shewchuk:1994:IntroToCG}.

To understand Conjugate Gradient, start by considering the Method of Steepest Decent.
This method takes an initial \(\vec{x}_0\) and repeatedly computes improved points, \(\vec{x}_1, \vec{x}_2, \dots\), until reaching a point close enough to the minimum of Equation~\ref{eq:quad-form}~\cite{Nearing:2010:toolsForPhysics}.
Because the gradient at a point is the direction of maximal increase, 
\begin{equation}
\label{eq:cg-xUpdate}
	\vec{x}_{i+1} = \vec{x}_i + \alpha_i \vec{r}_i
\end{equation}
for some \(\alpha_i > 0\) and where \(\vec{r}_i = -\nabla f\left(\vec{x}_i\right) = \vec{b} - \mat{A}\vec{x}_i\) is the residual of \(\vec{x}_i\).
So, the value for \(\alpha_i\) that minimizes \(f\left(\vec{x}_{i+1}\right)\) occurs at the bottom of the parabola formed when taking \(\vec{x}_{i+1}\) as a function of \(\alpha_i\).
Thus, choose \(\alpha_i\) such that
\begin{align*}
	0
	&= \frac{\mathrm{d}}{\mathrm{d} \alpha_i} f\left(\vec{x}_{i+1}\right) \\
	&= \frac{\mathrm{d}}{\mathrm{d} \alpha_i} f\left(\vec{x}_i + \alpha \vec{r}_i\right) \\
	&= \nabla f\left(\vec{x}_i+\alpha_i\vec{r}_i\right) \cdot \vec{r}_i \\
	&= \left(\mat{A}\left(\vec{x}_i+\alpha_i\vec{r}_i\right)-\vec{b}\right)\cdot \vec{r}_i \\
	&= \left(\mat{A}\vec{x}_i-\vec{b}\right)\cdot \vec{r}_i + \alpha_i\mat{A}\vec{r}_i\cdot \vec{r}_i\\
%
	-\alpha_i\mat{A}\vec{r}_i\cdot \vec{r}_i
	&= -\vec{r}_i\cdot \vec{r}_i \\
%
	\alpha_i
	&= \frac{\vec{r}_i\cdot \vec{r}_i}{\vec{r}_i\cdot\mat{A}\vec{r}_i}.
\end{align*}
Note that by premultiplying both sides of Equation~\ref{eq:cg-xUpdate} by \(-\mat{A}\) then adding \(\vec{b}\), the residual can be computed
\[
	\vec{r}_{i+1} = \vec{r}_i - \alpha\mat{A}\vec{r}_i
\].
The resulting steps for the Method of Steepest Decent are
%TODO consider switching to proper algorithm
\begin{align*}
	\vec{r}_0 &= \vec{b} - \mat{A}\vec{x}_0 \\
	\alpha_i &= \frac{\vec{r}_i\cdot \vec{r}_i}{\vec{r}_i\cdot\mat{A}\vec{r}_i}\\
	\vec{x}_{i+1} &= \vec{x}_i + \alpha_i\vec{r}_i\\
	\vec{r}_{i+1} &= \vec{r}_i - \alpha\mat{A}\vec{r}_i
\end{align*}
until \(\left\|\vec{r}_i\right\|\) is less than some tolerance~\cite{Shewchuk:1994:IntroToCG}.


\begin{example}
\label{ex:CG-SteepestDecent}
%x = (2, 1)
Consider the linear system
\[
	\mat{A} = \begin{bmatrix}
		2 & 1 \\
		1 & 3
	\end{bmatrix},
	\quad
	\vec{b} = \begin{bmatrix}
		5 \\
		5 
	\end{bmatrix}
\]
and use \(c=0\).
Note that the solution is
\[
	\vec{x} = \begin{bmatrix}
		2 \\ 1
	\end{bmatrix}.
\]
When starting at the origin, the iteration of Method of Steepest Decent becomes
\begin{align*}
	 \vec{x}_0 &= \begin{bmatrix}0\\0\end{bmatrix}
	&\vec{r}_0 &= \begin{bmatrix}5\\5\end{bmatrix}
	&\alpha_0  &= 2/7 \\
	%
	 \vec{x}_1 &= \begin{bmatrix}10/7\\10/7\end{bmatrix}
	&\vec{r}_1 &= \begin{bmatrix}5/7\\-5/7\end{bmatrix}
	&\alpha_1  &= 2/3 \\
	%
	 \vec{x}_2 &= \begin{bmatrix}40/21\\20/21\end{bmatrix}
	&\vec{r}_2 &= \begin{bmatrix}5/21\\5/21\end{bmatrix}
	&\alpha_2  &= 2/7 \\
	%
	 \vec{x}_3 &= \begin{bmatrix}290/147\\50/49\end{bmatrix}
	&\vec{r}_3 &= \begin{bmatrix}5/147\\-5/147\end{bmatrix}
	&\alpha_3  &= 2/3 \\
	%
	&\vdots &&\vdots &&\vdots
\end{align*}
The \(\vec{x}_i\)'s are plotted with a contour graph of the quadratic form in Figure~\ref{fig:CG-steepestDecentExample-Iterations}.
\input{"figures/2.Background/Conjugate Gradient/Steepest Decent Example-Iterations.tex"}
\end{example}

%Conjugate Directions
Next, consider Conjugate Directions, a class of iterative linear solvers, of which Conjugate Gradient is a member of~\cite{Shewchuk:1994:IntroToCG}.
Note that, in Example~\ref{ex:CG-SteepestDecent}, the directions of \(\vec{r}_0\) and \(\vec{r}_2\) are the same and the directions of \(\vec{r}_1\) and \(\vec{r}_3\) are the same.
Thus, the same direction has to be traversed multiple times.
Additionally, note that the two sets of residual directions are perpendicular to each other.
Conjugate Directions attempts to improve on this, by making the search directions, \(\vec{d}_0, \vec{d}_1, \dots\), \(\mat{A}\)-orthogonal to each other and only moving \(\vec{x}\) once in each search direction.
Two vectors, \(\vec{u}, \vec{v}\) are \(\mat{A}\)-orthogonal, or conjugate, if \(\vec{u}^T\mat{A}\vec{v}=0\).
The requirement for Conjugate Directions is to make \(\vec{e}_{i+1}\) \(\mat{A}\)-orthogonal to \(\vec{d}_i\), where \(\vec{e}_i = \vec{x}_i - \mat{A}^{-1}\vec{b}\) is the error of the current \(\vec{x}\).
Then, we can derive
\[
	\alpha_i = \frac{\vec{d}_i^T\vec{r}_i}{\vec{d}_i^T\mat{A}\vec{d}_i}.
\]

%Conjugate Gradient proper
%TODO cite Intro to CG more aggressively
Conjugate Gradient is a form of Conjugate Directions where the residuals are made \(\mat{A}\)-orthogonal to each other.
The conjugate Gram-Schmidt process is used to make the residuals \(\mat{A}\)-orthogonal.
To do this, each search direction, \(\vec{d}_i\) is computed by taking \(\vec{r}_i\) and removing any components that are not \(\mat{A}\)-orthogonal to the previous \(\vec{d}\)'s.
So, let \(\vec{d}_0 = \vec{r}_0\) and for \(i > 0\) let
\[
	\vec{d}_i = \vec{r}_i + \sum_{k=0}^{i-1} \beta_{ik}\vec{d}_k
\]
with \(\beta_{ik}\) defined for \(i > k\).
Then, \(\beta_{ik}\) can be solved for, giving
\[
	\beta_{ik} = -\frac{\vec{r}_i^T\mat{A}\vec{d}_i}{\vec{d}_j^T\mat{A}\vec{d}_j}.
\]
Note that each residual is orthogonal to the previous search directions and thus previous residuals.
So, it can be shown that \(\vec{r}_{i+1}\) is \(\mat{A}\)-orthogonal to all previous search directions, except \(\vec{d}_i\)~\cite{Shewchuk:1994:IntroToCG}.
Then, \(\beta_{ik} = 0\) for \(i \neq k\).
To simplify notation, let \(\beta_i = \beta_{i,i-1}\).
So, each new search direction can then be computed by
\[
	\vec{d}_i = \vec{r}_i + \beta_i\vec{d}_{i-1}.
\]
This results in the following algorithm for Conjugate Gradient
%TODO consider switching to proper algorithm
\begin{align*}
	\vec{d}_0 = \vec{r}_0 &= \vec{b} - \mat{A}\vec{x}_0 \\
	\alpha_i &= \frac{\vec{r}_i\cdot\vec{r}_i}{\vec{d}_i\cdot \mat{A}\vec{d}_i} \\
	\vec{x}_{i+1} &= \vec{x}_i + \alpha_i\vec{r}_i \\
	\vec{r}_{i+1} &= \vec{r}_i - \alpha\mat{A}\vec{r}_i \\
	\beta_{i+1} &= \frac{\vec{r}_{i+1}\cdot\vec{r}_{i+1}}{\vec{r}_i\cdot\vec{r}_i} \\
	\vec{d}_{i+1} &= \vec{r}_{i+1} + \beta_{i+1}\vec{d}_i
\end{align*}