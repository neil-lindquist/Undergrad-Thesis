
To help understand the requirements to seen an improvement in overall performance, models were constructed to estimate the amount of time spent fetching and decoding information.
Two models were constructed, an analytical model that did not take into account processor level parallelism and a simulation based model one that took into account certain processor level parallelism.
Both models are based on the sparse matrix-vector product, but, due to the similarity of the data access for the symmetric Gauss-Seidel step, should provide an estimate on both kernals.
Additionally, the models only provide estimates for rows with 27 elements, because there are \(O(n^3)\) of those rows and only \(O(n^2)\) of other rows where the matrix has \(O(n^3)\) rows.
Finally, the models assume that the compression does not reduce the rate of convergence.
The models were analyzed using the memory access latencies of the head node of the testing cluster.
These latencies are shown in Table~\ref{tab:models-baseLatencies}.
The models were primarily used to find the minimum compression performance to outperform the baseline implementation.
The following variables represent the relevant compression characteristics in this section
\begin{align*}
	\mathrm{vectDecode} &= \text{time to decode one vector value} \\
	\mathrm{vectEncode}  &= \text{time to encode one vector value} \\
	\mathrm{vectBytes} &= \text{the number of bytes per vector value} \\
	\mathrm{matIndDecode} &= \text{time to decode one matrix index} \\
	\mathrm{matIndBytes} &= \text{the number of bytes per matrix index} \\
	\mathrm{matValDecode} &= \text{time to decode one matrix value} \\
	\mathrm{matValBytes} &= \text{the number of bytes per matrix value}
\end{align*}

\input{figures/2b.Models/baseLatencies}

\subsection{Analytical Model}

The analytical model is a set of equations that computes the amount of time spent serially fetching and decoding the information.
This model is implemented using the following system of equations
\begin{align*}
	& 27\cdot \mathrm{vectDecode}+\mathrm{vectEncode} + 18\cdot \mathrm{L1Time} \\
	+ & \left(\frac{64-\mathrm{vectBytes}}{64}\cdot 9\cdot \mathrm{L1Time}+\frac{\mathrm{vectBytes}}{64}\cdot \left(6\cdot \mathrm{L2Time}+3\cdot \mathrm{RAMTime}\right)\right) \\
	+ & 27\cdot\mathrm{matIndDecode} \\
	+ & 27\cdot\left(\frac{\mathrm{matIndBytes}}{64}\cdot\mathrm{RAMTime} + \frac{64-\mathrm{matIndBytes}}{64}\cdot\mathrm{L1Time}\right) \\
	+ & 27\cdot\mathrm{matValDecode} \\
	+ & 27\cdot\left(\frac{\mathrm{matValBytes}}{64}\cdot\mathrm{RAMTime}+\frac{64-\mathrm{matValBytes}}{64}\cdot\mathrm{L1Time}\right)
\end{align*}
where
\begin{align*}
	\mathrm{L1Time} &= \text{the access latency for L1 cache} \\
	\mathrm{L2Time} &= \text{the access latency for L2 cache} \\
	\mathrm{RAMTime} &= \text{the access latency for main memory.} \\
\end{align*}
This model utilizes a few facts.
%TODO add nessaccery caveats about problem size
Firstly, the number of bytes per value divided by 64 provides the percent of values that will require fetching a cacheline from main memory or higher caches, while 1 minus this values is the percent of values that will be able to only need to access L1 cache.
Secondly, due the matrix sparsity pattern, two thirds of vector values will always be in L1 cache and two thirds of the remaining values will be in L2 cache.
The model was only studied using the performance characteristics shown in Table~\ref{tab:models-baseLatencies}.
Outperforming the baseline implementation in this model requires meeting the approximate bounds
\begin{align*}
\mathrm{decode} =& \mathrm{vectDecode} + \mathrm{matValDecode} + \mathrm{matIndDecode} \\
\mathrm{matBytes} =& \mathrm{matValBytes} + \mathrm{matIndBytes}\\
\\
\mathrm{vectEncode} <& 878.513 \\
\mathrm{decode} <& 32.5375 - 0.037037\cdot\mathrm{vectEncode} \\
\mathrm{matBytes} <& 12.9664 - 0.398506\cdot\mathrm{decode}- 0.0147595\cdot\mathrm{vectEncode}\\
\mathrm{vectBytes} <& 107.34  - 3.29897\cdot\mathrm{decode} - 0.122184\cdot\mathrm{vectEncode} \\
	&- 8.27835\cdot\mathrm{matBytes}\\
\end{align*}
where all encode and decode times are in clocks.
Note that the upper bound on the number of bytes per vector value is reduced by 3.29897 per 1 clock increase in decode time.
This indicates that only highly effective compression techniques will be effective for vector values.
Matrix compression on the other hand, appears to be able to achieve a performance improvement with a slower decompression than required by the vector values.

\subsection{Simulation Based Model}
The parallel model attempts to estimate the performance required to outperform the baseline implementation while taking into account processor level parallelism.
Figure~\ref{fig:models-dataDeps} shows the dependencies used by each model.
Note that this model assumes that the compiler and processor can fully parallelize any operations without data dependencies, while, at the absolute minimum, the instructions must be read serially~\cite{Hennessy:1990:ComputerArchitecture}.
Additionally, the model assumes that the bytes for each compressed value is constant, which is not true for most matrix compression and for some vector compression.
Lastly, the model was only worked with using integral values for the bytes and decode/encode times.
The model then takes the same compression properties as the first model and computes the time to fetch and decode the values and encode the result value over 10 matrix rows with 27 entries per row.
Appendix~\ref{app:decode-model-source} contains source code for this model.

\input{figures/2b.Models/dataDependacies}

The bounds on outperforming the baseline implementation were hard to determine for the parallel model due to the nature of the model as a sum of maximizations.
However, some boundaries were computed.
When only compressing vectors, the compression needs to perform such that
\[
	8 \geq \mathrm{vectBytes} + \tfrac{7}{6}\cdot \mathrm{vectDecode} + \tfrac{2}{3} \cdot \mathrm{vectEncode}.
\]
This model allows for a slightly less efficient decoding step than the analytical model, however encoding vectors must be more efficient for this model.
The bounds on matrix compression are less intuitive.
Table~\ref{tab:models-singleCompMatrix} shows the maximum decode times for matrix indices and values respectively.
These compression bounds appear to be significantly related to the frequency at which multiple values are fetched from main memory.

%TODO figure out a way to think about/work with multiple compressions at once

\input{figures/2b.Models/singleCompMatrixTimings}


