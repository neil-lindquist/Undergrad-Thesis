Tables~\ref{tab:results-vec}, \ref{tab:results-val} and~\ref{tab:results-ind} show the compression results for compressing just the vector values, matrix values and matrix indices respectively.
These tables, and all following tables of test results, contain the rating measured by HPCG, the GFLOP rating with convergance overhead, the number of iterations needed for convergence and, where computed, the compression rate based on the number of cache lines fetched, which may be different then the memory allocated.
Note that some compression strategies had multiple variations that were tested.
Section~\ref{sec:results-vec} contains information on variations used in vector compression.
The compression of just one data structure fails to outperform the baseline implementation; Section~\ref{sec:results-bounds} discusses this further.

%TODO make note of consistancy results (10 runs of baseline had the range of ratings equal to .6128)
%TODO discuss why compiler settings don't have a significant affect on things
	%Consider putting consistancy with compiler settings, both about why these results are good enough
%TODO considering using rating w/out optimization overhead also/instead
%TODO make sure that the results shown don't need to be cut down to be understandable

\input{figures/4.Results/Vec.tex}
\input{figures/4.Results/Val.tex}
\input{figures/4.Results/Ind.tex}

Next, combined compression schemes were tried, using SZ and single precision compression for the matrix values and using SZ, gamma and delta compression for the matrix indices.
Table~\ref{tab:results-combined-mat} shows the results of these combined schemes.
The combined scheme with the best performance used SZ compression for both values and indices.
The only other approach that outperformed the baseline implementation used 32 bit compression for the values and gamma compression for the indices.

\input{figures/4.Results/Combined-mat.tex}

Finally, vector compression was combined with the successful combined matrix compression.
Due to the poor performance of SZ and ZFP vector compression, only the better versions of mixed precision vector compression were used.
Table~\ref{tab:results-combined-vec+mat} shows the results for these compression strategies.
The first column indicates which vectors were stored in 32bit; the rest of the columns correspond to their counter parts in Table~\ref{tab:results-combined-mat}.
Note that vector compression improved the performance of the SZ compressed matrices, but reduced the performance of the gamma compressed matrices.
So, the best implementation for the test problem uses mixed precision vectors with \(\vec{b}, \vec{x}\) and \(\mat{A}\vec{d}\) stored in single precision, and sz compressed matrix values and indices.

\input{figures/4.Results/Combined-vec+mat.tex}

\subsection{Performance Improvement Bounds}
\label{sec:results-bounds}
Note that Table~\ref{tab:results-val} shows 1 bit compression under performing the baseline implementation, even though it has a significant compression rate.
This demonstrates that compressing the matrix values alone in unable to improve performance.
For the vector values, note that the single precision implementation has a 2.3 times increase in iterations to convergence over the baseline implementation and that the GFLOPs rating of the single precision implementation is reduced by a factor of approximately 2.19 from the baseline implementation.
This hints that, even without increasing the number of Conjugate Gradient iterations, compressing the vectors requires a compression rate better than 1:2 to provide much of an improvement in performance.
This analysis is supported by the fact that none of the compression strategies tried that only compressed a single strategy where able to out perform the baseline implementation.

\subsection{Vector Compression}
\label{sec:results-vec}
As shown in Table~\ref{tab:results-vec}, vector compression was not successfully used to improve performance.
Section~\ref{sec:results-bounds} discusses why performance improvement is likely limited.
However, vector compression is able to make improvements when combined with other compressions, as shown in Table~\ref{tab:results-combined-vec+mat}.

ZFP had poor performance when compressing vector information.
The high level array API was used to provide the necessary random access.
Note that 1 dimensional ZFP compression has a 16 bit granularity, and 3 dimensional ZFP compression has a 1 bit granularity~\cite{Lindstrom:2014:zfp}.
These granularity restrictions and the resulting iterations needed were used to select the tested compression rates.

SZ compression has two main configurable settings, the number of values in each block and the error bound.
There were two measures of error that were considered, absolute error and pointwise relative error.
The performance was tested with both a single error being bounded and both errors being bounded.
Absolute error is the absolute value of the difference between predicted and actual.
The pointwise relative error is the absolute error divided by the actual value.
Table~\ref{tab:results-vec} contains results for various block sizes with both an absolute error bound of \(10^{-10}\) and a pointwise relative error bound of \(10^{-10}\).
Table~\ref{tab:results-vec-SZ} contains an comparison of various error bounds for a block size of 8 values per block.
%TODO get timings with another block size (12 val blocks?)
Note that an absolute bound of \(10^{-2}\) was unable to converge within 500 iterations.
%TODO is this note useful?

\input{figures/4.Results/Vec-SZ.tex}

\subsection{Testing Environment}
The timings presented were obtained when using a problem size of \(96^3\) matrix rows.
The cluster used for timings had a 20 core, 2.2 GHz Intel Xeon E5-2698 v4 head node and an additional five 8 core, 1.7GHz Intel Xeon E5-2605 nodes.
One process was created for each core, with a single OMP thread per process.

The code was implemented using version 3.0.0 of the HPCG benchmark~\cite{Dongarra:2015:HPCG}.
The code was compiled with the OpenMPI cxx wrapper using GCC version 4.8.5.
OpenMPI 3.2 was used for the compiler wrapper and MPI runtime.
The O3 and fopenmp flags were used for compilation, in addition to a selection of warning flags and the std flag as necessary.