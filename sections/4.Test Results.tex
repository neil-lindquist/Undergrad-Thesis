Tables~\ref{tab:results-vec}, \ref{tab:results-val} and~\ref{tab:results-ind} show the compression results for compressing just the vector values, matrix values and matrix indices respectively.
These tables, and all following tables of test results, contain the rating measured by HPCG, the GFLOP rating with convergence overhead, the number of iterations needed for convergence and, where computed, the compression rate based on the number of cache lines fetched, which may be different then the memory allocated.
Note that some compression strategies had multiple variations that were tested.
Section~\ref{sec:results-vec} contains information on variations used in vector compression.
The compression of just one data structure fails to outperform the baseline implementation; Section~\ref{sec:results-bounds} discusses this further.

Note that for comparing times, ten runs of the baseline implementation on the standard test settings had a range of 0.6128 and a standard deviation of 0.1846 for the HPCG rating.
For the effective GFLOPs, there was a range of 0.4629 and a standard deviation of 0.1504.
Thus, when comparing similar values, it should be noted that the values may vary by a few tenths of a GFLOP between runs.
%TODO wording

%TODO make sure that the results shown don't need to be cut down to be understandable

\input{figures/4.Results/Vec.tex}
\input{figures/4.Results/Val.tex}
\input{figures/4.Results/Ind.tex}

Next, combined compression schemes were tried, using SZ and single precision compression for the matrix values and using SZ, gamma and delta compression for the matrix indices.
Table~\ref{tab:results-combined-mat} shows the results of these combined schemes.
The combined scheme with the best performance used SZ compression for both values and indices.
The only other approach that outperformed the baseline implementation used 32-bit compression for the values and gamma compression for the indices.

\input{figures/4.Results/Combined-mat.tex}

Finally, vector compression was combined with the successful combined matrix compression.
Due to the poor performance of SZ and ZFP vector compression, only the better versions of mixed precision vector compression were used.
Table~\ref{tab:results-combined-vec+mat} shows the results for these compression strategies.
The first column indicates which vectors were stored in 32bit; the rest of the columns correspond to their counter parts in Table~\ref{tab:results-combined-mat}.
Note that vector compression improved the performance of the SZ compressed matrices but reduced the performance of the gamma compressed matrices.
So, the best implementation for the test problem uses mixed precision vectors with \(\vec{b}, \vec{x}\) and \(\mat{A}\vec{d}\) stored in single precision, and sz compressed matrix values and indices.

\input{figures/4.Results/Combined-vec+mat.tex}

\subsection{Performance Improvement Bounds}
%TODO figure out if this should be it's own section, or combined with data structure specific sections
\label{sec:results-bounds}
Note that Table~\ref{tab:results-val} shows 1-bit compression underperforming the baseline implementation, even though it has a significant compression rate.
This demonstrates that compressing the matrix values alone in unable to improve performance.
For the vector values, note that the single precision implementation has a 2.3 times increase in iterations to convergence over the baseline implementation and that the GFLOPs rating of the single precision implementation is reduced by a factor of approximately 2.19 from the baseline implementation.
This hints that, even without increasing the number of Conjugate Gradient iterations, vector compression requires a compression rate better than 1:2 to provide much of an improvement in performance.
This analysis is supported by the fact that none of the compression strategies tried that only compressed a single strategy were able to outperform the baseline implementation.

\subsection{Vector Compression}
\label{sec:results-vec}
As shown in Table~\ref{tab:results-vec} and discussed in Section~\ref{sec:results-bounds}, vector compression was not successfully used to improve performance.
However, certain combinations of single and double precision vector values were able to perform close to the baseline performance.
These compression combinations were able to make improvements when combined with other compressions, as shown in Table~\ref{tab:results-combined-vec+mat}.

ZFP had poor performance when compressing vector information.
The high level array API was used to provide the necessary random access.
That API provides an adjustable cache for decoded values.
Figure~\ref{fig:results-zfp-cache} shows the performance of 1d ZFP compression versus the cache size.
Note that the two large jumps occur when the values only need to be decoded once per matrix-vector product and when the values can be left permanently decoded.
The results in Table~\ref{tab:results-vec} use the default cache size of 2 blocks for 1 dimensional compression and \({2\cdot\ceil{nz/2}\cdot\ceil{ny/2}}\) blocks for 3 dimensional compression.
The array API also allows selecting the compression rate, with a 16 bit granularity for 1-dimensional compression and a 1 bit granularity for 3-dimensional compression~\cite{Lindstrom:2014:zfp}.
These granularity restrictions and the resulting iterations needed were used to select the tested compression rates.

\input{figures/4.Results/Vec-zfp-cache.tex}

SZ compression has two main configurable settings, the number of values in each block and the error bound.
There were two measures of error that were considered, absolute error and pointwise relative error.
The performance was tested with both a single error being bounded and both errors being bounded.
Absolute error is the absolute value of the difference between predicted and actual.
The pointwise relative error is the absolute error divided by the actual value.
Table~\ref{tab:results-vec} contains results for various block sizes with both an absolute error bound of \(10^{-10}\) and a pointwise relative error bound of \(10^{-10}\).
Table~\ref{tab:results-vec-SZ} contains a comparison of various error bounds for a block size of 8 values per block.
%TODO get timings with another block size (12 val blocks?)
Note that an absolute bound of \(10^{-2}\) was unable to converge within 500 iterations.
%TODO is this note useful?

\input{figures/4.Results/Vec-SZ.tex}

\subsection{Matrix Value Compression}
Like vector compression, matrix value compression alone was unable to outperform the baseline implementation.
As discussed in Section~\ref{sec:results-bounds}, 1-bit compression was unable to outperform the baseline implementation, which show that a reduction in the size of matrix values alone provides little benefit.
Both SZ compression and single precision compression were not significantly under the baseline, indicating that they may be usable in conjunction with other compression techniques.
On the other hand, ZFP compression performed over a magnitude slower than the baseline, making it unlikely that it could improve performance, even when combined with other techniques.

As mentioned in Section~\ref{sec:bg-comp-sz}, there were two possible sets of curves that could be used for matrix value compression.
One setup uses only the Neighbor curve, and the other setup uses both the Neighbor and Neighbor's Neighbor curve.
%TODO double check compression rates, then compare compression rates of the different modes
Both setups performed similarly, with the 2-curve version performing slightly better.
%TODO need justification why both weren't tested in combined compression
The one curve version was used when combining with other compression methods.

%TODO look at the level of effort applied to ZFP
%Should probably reimplement ZFP matVal versions to be variable rate encoded w/ a forward and backward iterations
%Or just drop ZFP matVals b/c technically not designed for the type of data that is being compressed
ZFP compression had multiple configurations that could be used.
However, the possible configurations were not fully experimented with since the matrix values lacks the spacial relations that ZFP is designed for and ZFP performs poorly on the configurations that are tested, for both vector and matrix value compression.
First, unlike ZFP vector compression, only the 1-dimensional codec was tested because the data does not have spacial patterns, let alone multiple dimensions of them.
Second, due to the simple access pattern for matrix values, both the high-level array API and the low level API were tried.
The compression rate was kept at 32 bits per value.
None of the configurations tests were able to provide even mediocre performance, so ZFP compression was not tested further for matrix compression.

\subsection{Testing Environment}
The timings presented were obtained when using a problem size of \(96^3\) matrix rows.
The cluster used for timings had a 20-core, 2.2 GHz, Intel Xeon E5-2698 v4 head node and an additional five 8-core, 1.7GHz, Intel Xeon E5-2605 nodes.
One process was created for each core, with a single OMP thread per process.

The code was implemented using version 3.0.0 of the HPCG benchmark~\cite{Dongarra:2015:HPCG}.
Many of the implementations with timings listed in this paper can be found at \url{https://github.com/Collegeville/HPCG-ZFP}~\cite{Lindquist:2018:projectGithub}.
The code was compiled with the OpenMPI cxx wrapper using GCC version 4.8.5.
OpenMPI 3.2 was used for the compiler wrapper and MPI runtime.
The \texttt{O3} and \texttt{fopenmp} flags were used for compilation, in addition to a selection of warning flags and the \texttt{std} flag as necessary.
No \texttt{HPCG\_OPTS} flags were enabled.